{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "\n",
    "# importing required libraries\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import PipelineModel\n",
    "import datetime as dt\n",
    "from pyspark.sql.functions import col, concat, expr, explode, split, regexp_replace\n",
    "from pyspark.sql.types import LongType, StringType, StructField, StructType, BooleanType, ArrayType, IntegerType, DoubleType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.1\n",
    "# running Spark with 2 logical cores\n",
    "master = \"local[2]\"\n",
    "# setting the appname\n",
    "app_name = \"Hacking Analysis\"\n",
    "# setting spark's configuration parameters\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "# Building SparkSession with Timezone as UTC\n",
    "spark = SparkSession.builder.config(conf=spark_conf)\\\n",
    "            .config(\"spark.sql.session.timeZone\", 'UTC')\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.2\n",
    "# subscribe to topic memory\n",
    "topic = \"Memory\"\n",
    "# connect to the kafka server for memory data\n",
    "memory_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# raw schema \n",
    "memory_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.2\n",
    "# subscribe to topic process\n",
    "topic = \"Process\"\n",
    "# connect to the kafka server for process data\n",
    "process_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw schema\n",
    "process_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.3\n",
    "# cast the values and keys as string\n",
    "memory_df = memory_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw schema\n",
    "memory_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.3\n",
    "# cast the values and keys as string\n",
    "process_df = process_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw schema\n",
    "process_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.3\n",
    "# memory schema\n",
    "mem_schema = ArrayType(StructType([\n",
    "    StructField(\"sequence\", StringType(), True),\n",
    "    StructField(\"machine\", StringType(), True),\n",
    "    StructField(\"PID\", StringType(), True),\n",
    "    StructField(\"MINFLT\", StringType(), True),\n",
    "    StructField(\"MAJFLT\", StringType(), True),\n",
    "    StructField(\"VSTEXT\", StringType(), True),\n",
    "    StructField(\"VSIZE\", StringType(), True),\n",
    "    StructField(\"RSIZE\", StringType(), True),\n",
    "    StructField(\"VGROW\", StringType(), True),\n",
    "    StructField(\"RGROW\", StringType(), True),\n",
    "    StructField(\"MEM\", StringType(), True),\n",
    "    StructField(\"CMD\", StringType(), True),\n",
    "    StructField(\"ts\", TimestampType(), True)\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.3\n",
    "# process schema\n",
    "pro_schema = ArrayType(StructType([\n",
    "    StructField(\"sequence\", StringType(), True),\n",
    "    StructField(\"machine\", StringType(), True),\n",
    "    StructField(\"PID\", StringType(), True),\n",
    "    StructField(\"TRUN\", StringType(), True),\n",
    "    StructField(\"TSLPI\", StringType(), True),\n",
    "    StructField(\"TSLPU\", StringType(), True),\n",
    "    StructField(\"POLI\", StringType(), True),\n",
    "    StructField(\"NICE\", StringType(), True),\n",
    "    StructField(\"PRI\", StringType(), True),\n",
    "    StructField(\"RTPR\", StringType(), True),\n",
    "    StructField(\"CPUNR\", StringType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"EXC\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"CPU\", StringType(), True),\n",
    "    StructField(\"CMD\", StringType(), True),\n",
    "    StructField(\"ts\", TimestampType(), True)\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.3\n",
    "# unnesting the data and renaming the columns\n",
    "memory_df = memory_df.select(F.from_json(F.col(\"value\").cast(\"string\"), mem_schema).alias('parsed_value'))\n",
    "memory_df = memory_df.select(F.explode(F.col(\"parsed_value\")).alias('unnested_value'))  \n",
    "memory_df = memory_df.select(\n",
    "                    F.col(\"unnested_value.sequence\").alias(\"sequence\"),\n",
    "                    F.col(\"unnested_value.machine\").alias(\"machine\"),\n",
    "                    F.col(\"unnested_value.PID\").alias(\"PID\"),\n",
    "                    F.col(\"unnested_value.MINFLT\").alias(\"MINFLT\"),\n",
    "                    F.col(\"unnested_value.MAJFLT\").alias(\"MAJFLT\"),\n",
    "                    F.col(\"unnested_value.VSTEXT\").alias(\"VSTEXT\"),\n",
    "                    F.col(\"unnested_value.VSIZE\").alias(\"VSIZE\"),\n",
    "                    F.col(\"unnested_value.RSIZE\").alias(\"RSIZE\"),\n",
    "                    F.col(\"unnested_value.VGROW\").alias(\"VGROW\"),\n",
    "                    F.col(\"unnested_value.RGROW\").alias(\"RGROW\"),\n",
    "                    F.col(\"unnested_value.MEM\").alias(\"MEM\"),\n",
    "                    F.col(\"unnested_value.CMD\").alias(\"CMD\"),\n",
    "                    F.col(\"unnested_value.ts\").alias(\"ts\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.3\n",
    "# changing the datatype as per the metadata\n",
    "memory_df = memory_df.withColumn(\"sequence\",col(\"sequence\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"machine\",col(\"machine\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"PID\",col(\"PID\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"MINFLT\",col(\"MINFLT\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"MAJFLT\",col(\"MAJFLT\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"VSTEXT\",col(\"VSTEXT\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"VSIZE\",col(\"VSIZE\").cast(DoubleType())) \\\n",
    "                    .withColumn(\"RSIZE\",col(\"RSIZE\").cast(DoubleType())) \\\n",
    "                    .withColumn(\"VGROW\",col(\"VGROW\").cast(DoubleType())) \\\n",
    "                    .withColumn(\"RGROW\",col(\"RGROW\").cast(DoubleType())) \\\n",
    "                    .withColumn(\"MEM\",col(\"MEM\").cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# final schema\n",
    "memory_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.3\n",
    "# unnesting the data and renaming the columns\n",
    "process_df = process_df.select(F.from_json(F.col(\"value\").cast(\"string\"), pro_schema).alias('parsed_value'))\n",
    "process_df = process_df.select(F.explode(F.col(\"parsed_value\")).alias('unnested_value'))  \n",
    "process_df = process_df.select(\n",
    "                    F.col(\"unnested_value.sequence\").alias(\"sequence\"),\n",
    "                    F.col(\"unnested_value.machine\").alias(\"machine\"),\n",
    "                    F.col(\"unnested_value.PID\").alias(\"PID\"),\n",
    "                    F.col(\"unnested_value.TRUN\").alias(\"TRUN\"),\n",
    "                    F.col(\"unnested_value.TSLPI\").alias(\"TSLPI\"),\n",
    "                    F.col(\"unnested_value.TSLPU\").alias(\"TSLPU\"),\n",
    "                    F.col(\"unnested_value.POLI\").alias(\"POLI\"),\n",
    "                    F.col(\"unnested_value.NICE\").alias(\"NICE\"),\n",
    "                    F.col(\"unnested_value.PRI\").alias(\"PRI\"),\n",
    "                    F.col(\"unnested_value.RTPR\").alias(\"RTPR\"),\n",
    "                    F.col(\"unnested_value.CPUNR\").alias(\"CPUNR\"),\n",
    "                    F.col(\"unnested_value.Status\").alias(\"Status\"),\n",
    "                    F.col(\"unnested_value.EXC\").alias(\"EXC\"),\n",
    "                    F.col(\"unnested_value.State\").alias(\"State\"),\n",
    "                    F.col(\"unnested_value.CPU\").alias(\"CPU\"),\n",
    "                    F.col(\"unnested_value.CMD\").alias(\"CMD\"),\n",
    "                    F.col(\"unnested_value.ts\").alias(\"ts\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.3\n",
    "# changing the datatype as per the metadata\n",
    "process_df = process_df.withColumn(\"sequence\",col(\"sequence\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"machine\",col(\"machine\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"PID\",col(\"PID\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"TRUN\",col(\"TRUN\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"TSLPI\",col(\"TSLPI\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"TSLPU\",col(\"TSLPU\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"NICE\",col(\"NICE\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"PRI\",col(\"PRI\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"RTPR\",col(\"RTPR\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"CPUNR\",col(\"CPUNR\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"EXC\",col(\"EXC\").cast(IntegerType())) \\\n",
    "                    .withColumn(\"CPU\",col(\"CPU\").cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final schema\n",
    "process_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.3\n",
    "# Replacing K with 0000s\n",
    "# Using substring to remove K and typecast the data to int\n",
    "# Multiply by 1000 after typecasting\n",
    "memory_df = memory_df.withColumn('MINFLT',\\\n",
    "                     F.when(col(\"MINFLT\").contains('K'),\\\n",
    "                            expr(\"substring(MINFLT, 1, length(MINFLT)-1)\")\\\n",
    "                            .cast('int')*1000).otherwise(col(\"MINFLT\").cast('int')))\n",
    "\n",
    "# Replacing M with 000000s\n",
    "# Using substring to remove M and typecast the data to int\n",
    "# Multiply by 1000000 after typecasting\n",
    "memory_df = memory_df.withColumn('MAJFLT',\\\n",
    "                 F.when(col(\"MAJFLT\").contains('M'),\\\n",
    "                        expr(\"substring(MAJFLT, 1, length(MAJFLT)-1)\")\\\n",
    "                        .cast('double')*1000000).otherwise(col(\"MAJFLT\").cast('double')))\n",
    "\n",
    "# Replacing K with 0000s\n",
    "# Using substring to remove K and typecast the data to int\n",
    "# Multiply by 1000 after typecasting\n",
    "memory_df = memory_df.withColumn('VSTEXT',\\\n",
    "                 F.when(col(\"VSTEXT\").contains('K'),\\\n",
    "                        expr(\"substring(VSTEXT, 1, length(VSTEXT)-1)\")\\\n",
    "                        .cast('double')*1000).otherwise(col(\"VSTEXT\").cast('double')))\n",
    "\n",
    "# Replacing M with 000000s\n",
    "# Using substring to remove M and typecast the data to int\n",
    "# Multiply by 1000000 after typecasting\n",
    "memory_df = memory_df.withColumn('RSIZE',\\\n",
    "                 F.when(col(\"RSIZE\").contains('M'),\\\n",
    "                        expr(\"substring(RSIZE, 1, length(RSIZE)-1)\")\\\n",
    "                        .cast('double')*1000000).otherwise(col(\"RSIZE\")))\n",
    "\n",
    "# Replacing K with 0000s\n",
    "# Using substring to remove K and typecast the data to int\n",
    "# Multiply by 1000 after typecasting\n",
    "memory_df = memory_df.withColumn('RSIZE',\\\n",
    "                 F.when(col(\"RSIZE\").contains('K'),\\\n",
    "                        expr(\"substring(RSIZE, 1, length(RSIZE)-1)\")\\\n",
    "                        .cast('double')*1000).otherwise(col(\"RSIZE\").cast('double')))\n",
    "\n",
    "# Removing spaces from between numbers\n",
    "memory_df = memory_df.withColumn('VGROW', regexp_replace(col('VGROW'), ' ', ''))\n",
    "\n",
    "# Replacing K with 0000s\n",
    "# Using substring to remove K and typecast the data to int\n",
    "# Multiply by 1000 after typecasting\n",
    "memory_df = memory_df.withColumn('VGROW',\\\n",
    "                 F.when(col(\"VGROW\").contains('K'),\\\n",
    "                        expr(\"substring(VGROW, 1, length(VGROW)-1)\")\\\n",
    "                        .cast('double')*1000).otherwise(col(\"VGROW\").cast('double')))\n",
    "\n",
    "# Replacing K with 0000s\n",
    "# Using substring to remove K and typecast the data to int\n",
    "# Multiply by 1000 after typecasting\n",
    "memory_df = memory_df.withColumn('RGROW',\\\n",
    "                 F.when(col(\"RGROW\").contains('K'),\\\n",
    "                        expr(\"substring(RGROW, 1, length(RGROW)-1)\")\\\n",
    "                        .cast('double')*1000).otherwise(col(\"RGROW\").cast('double')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.3\n",
    "# NICE = PRI - 120 when NICE!=0 else PRI = 0\n",
    "process_df = process_df.withColumn('NICE', F.when(F.col('PRI') != 0, F.col('PRI') - 120).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.4\n",
    "# concatinating CMD and PID\n",
    "process_df = process_df.withColumn('CMD_PID', F.concat(process_df.CMD, F.lit('_'), process_df.PID))\n",
    "# creating event time\n",
    "process_df = process_df.withColumn('event_time', F.col('ts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.4\n",
    "# concatinating CMD and PID\n",
    "memory_df = memory_df.withColumn('CMD_PID', F.concat(memory_df.CMD, F.lit('_'), memory_df.PID))\n",
    "# creating event time\n",
    "memory_df = memory_df.withColumn('event_time', F.col('ts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.5\n",
    "# persisting the data in parquet format\n",
    "memory_sink = memory_df.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"parquet.memory\")\\\n",
    "        .option(\"checkpointLocation\", \"parquet.memory/checkpoint\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopping the sink\n",
    "memory_sink.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.5\n",
    "# persisting the data in parquet format\n",
    "process_sink = process_df.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"parquet.process\")\\\n",
    "        .option(\"checkpointLocation\", \"parquet.process/checkpoint\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopping the sink\n",
    "process_sink.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.6\n",
    "# loading the given machine learning models to predict attack events\n",
    "memory_mod = PipelineModel.load('memory_pipeline_model')\n",
    "# passing the data to the model to generate predictions\n",
    "memory_pred = memory_mod.transform(memory_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.6\n",
    "# loading the given machine learning models to predict attack events\n",
    "process_mod = PipelineModel.load('process_pipeline_model')\n",
    "# passing the data to the model to generate predictions\n",
    "process_pred = process_mod.transform(process_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.7 a\n",
    "# setting the watermark to 120 secs to count all entries having the SAME\n",
    "# “CMD_PID” in a 2-min window\n",
    "# checking for attacks only\n",
    "# groupping by machine and ts\n",
    "pro_count = process_pred \\\n",
    "    .withWatermark(\"ts\", \"120 seconds\") \\\n",
    "    .where('prediction==1')\\\n",
    "    .groupBy(F.window(process_df.ts, \"120 seconds\"),F.col('machine'))\\\n",
    "    .agg(F.approx_count_distinct(\"CMD_PID\").alias(\"total\"))\\\n",
    "    .select(\"window\",\"machine\",\"total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.7 a\n",
    "# viewing the output by writing to console\n",
    "query = pro_count \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"Complete\")\\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopping the query\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.7 a\n",
    "# writing the count to spark memory sink using complete output mode\n",
    "mem_ac_sink = pro_count \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"mem_ac\")\\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .option(\"truncate\",\"false\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopping the query\n",
    "mem_ac_sink.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.7 a\n",
    "# setting the watermark to 120 secs to count all entries having the SAME\n",
    "# “CMD_PID” in a 2-min window\n",
    "# checking for attacks only\n",
    "# groupping by machine and ts\n",
    "mem_count = memory_pred \\\n",
    "    .withWatermark(\"ts\", \"120 seconds\") \\\n",
    "    .where('prediction==1')\\\n",
    "    .groupBy(F.window(memory_df.ts, \"120 seconds\"),F.col('machine'))\\\n",
    "    .agg(F.approx_count_distinct(\"CMD_PID\").alias(\"total\"))\\\n",
    "    .select(\"window\",\"machine\",\"total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.7 a\n",
    "# viewing the output by writing to console\n",
    "query = mem_count \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"Complete\")\\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopping the query\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.7 a\n",
    "# writing the count to spark memory sink using complete output mode\n",
    "pro_ac_sink = pro_count \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"pro_ac\")\\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .option(\"truncate\",\"false\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopping the query\n",
    "pro_ac_sink.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.7 b\n",
    "# extracting the columns from memory predictions to be merged\n",
    "memory_uni = memory_pred\\\n",
    "            .selectExpr(\"MINFLT\", \"MAJFLT\", \"VSTEXT\", \"VSIZE\", \"RSIZE\", \"VGROW\",\\\n",
    "                        \"RGROW\", \"MEM\", \"CMD_PID AS M_CMD_PID\", \"ts AS M_ts\",\\\n",
    "                        \"event_time as M_event_time\", \"prediction AS M_prediction\")\\\n",
    "            .withWatermark(\"M_ts\", \"40 seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.7 b\n",
    "# extracting the columns from process predictions to be merged\n",
    "process_uni = process_pred\\\n",
    "            .selectExpr(\"TRUN\", \"TSLPI\", \"TSLPU\", \"POLI\", \"NICE\", \"PRI\", \"RTPR\",\\\n",
    "                        \"CPUNR\", \"Status\", \"EXC\", \"State\", \"CPU\", \"CMD_PID as P_CMD_PID\",\\\n",
    "                        \"ts as P_ts\", \"event_time as P_event_time\", \"prediction as P_prediction\")\\\n",
    "            .withWatermark(\"P_ts\", \"40 seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.7 b\n",
    "# joining the above two dataframes on CMD_PID where attack = 1 for both\n",
    "# and timestamp difference is no more than 30 seconds\n",
    "# REF - https://databricks.com/blog/2018/03/13/introducing-stream-stream-joins-in-apache-spark-2-3.html\n",
    "joint_df = process_uni.join(memory_uni,\\\n",
    "                               expr(\"\"\" \n",
    "                                    P_CMD_PID = M_CMD_PID AND \n",
    "                                    P_ts >= M_ts + interval 30 seconds AND \n",
    "                                    P_ts <= M_ts + interval 30 seconds AND\n",
    "                                    P_prediction = 1.0 AND\n",
    "                                    M_prediction = 1.0\n",
    "                                    \"\"\"\n",
    "                                  )\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.7 b\n",
    "# adding new column processing time to the joint df\n",
    "joint_df = joint_df.withColumn('processing_time', F.lit(int(dt.datetime.now().timestamp())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.7 b\n",
    "# viewing the output by writing to console \n",
    "query = joint_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"Append\")\\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the query\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.7 b\n",
    "# writing all the column in joint_df in parquet format\n",
    "joint_sink = joint_df.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"process_memory_attack.parquet\")\\\n",
    "        .option(\"checkpointLocation\", \"process_memory_attack.parquet/checkpoint\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopping the query\n",
    "joint_sink.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
